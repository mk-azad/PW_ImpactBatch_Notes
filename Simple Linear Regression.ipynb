{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf445856-7ec7-4e14-b3e6-2e75988506aa",
   "metadata": {},
   "source": [
    "Simple Linear Regression: Supervised Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc33fed-9572-4bc7-9435-039f11aee52e",
   "metadata": {},
   "source": [
    "- In supervised machine learning, we get two types of problem. (1) Regression and (2) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d42dd3-7b40-4fcb-803f-c94aa976942e",
   "metadata": {},
   "source": [
    "- In (1) Regression problem, the output/dependent feature that we have is a continues values\n",
    "- In (2) Classification problem, he output/dependent feature that we have is a binary/ multi class categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30856fbd-baad-4df8-8967-eaa57f66586a",
   "metadata": {},
   "source": [
    "In Simple linear regression, we have one independent feature and one dependent feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b5894-5671-4ea4-96d0-2d0322c53b3c",
   "metadata": {},
   "source": [
    "## The main aim of Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41e3b9-79db-467b-bfe0-6f34b8ae86c8",
   "metadata": {},
   "source": [
    "Lets assume we have a data set of wieghts and heights. We consider weights data points based on x-axis and heights data points based on y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1432d7-ae10-4e2b-9c3f-e80a7422ed5d",
   "metadata": {},
   "source": [
    "- To find out the best fit line in such a way that the summation of the error between the actual point (which are located near the best fit line) and the predicted point (which lies in the best fit line) should be minimal. \n",
    "- What does error means: When we calculate the best fit line, with respect to any new weight, we get a predicted height which fits in the best fit line. So, the distance between the actual point and the predicted point is known as an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e882c-7a8b-410e-bb84-e55e243293cc",
   "metadata": {},
   "source": [
    "## The purpose of finding a best fit line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15876f9a-7cb2-446d-83f1-10b85314e877",
   "metadata": {},
   "source": [
    "- Whenever a new data weight comes in the picture to predict the height, it has to be ploted in the best fit line. By this only, we can do the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02855324-18d6-430e-b3fb-222ac7350dd6",
   "metadata": {},
   "source": [
    "## How the best fit line is created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037d592-203d-4b54-bad8-1f5310cc5e22",
   "metadata": {},
   "source": [
    "There are some geometrical equation which is used to create the best fit line, like, y=mx+c, h(theta)x = (theta)0 + (theta)x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef02e8-8e4f-4e34-9828-8e917ba93fa9",
   "metadata": {},
   "source": [
    "- (Theta)0 = Intercept: It means that, when our x value is 0, where does the y axis meet.\n",
    "- (Theta)1 = Slope / Coefficient: It means that, with a unit movement in the x-axis how much movement do we have in the y-axis. In short, we are calculating the hypothenis of the slop using the concept of pythogerus theorum. \n",
    "\n",
    "- Example: Lets say we have experience at x-axis, and based on the experience we have salary at y-axis. When someone joins a job as a fresher, initially he/she will get a base package. Based on the experience, a base salary is fixed for a candidate. It is decided by the intercept.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a213c-a2de-427b-a2f1-c4ffe570b9a5",
   "metadata": {},
   "source": [
    "The above process happens when the model gets trained. We need to get the (Theta)0 and (Theta)1 to get the best fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67bcdf7-a6f9-46ff-96d8-552757c45e03",
   "metadata": {},
   "source": [
    "We initialize the (Theta)0 and (Theta)1 randomly to get a best fit line. Our main is to continuesly initialize this (Theta)0 and (Theta)1 value unless and untill we get a best fit line between the datapoints. Everytime we get a line, there will be an error. We compute the error. We choose the (Theta)0 and (Theta)1 value and the best fit line based on the minimum error that we get comparing to all other lines. We do this process by changing the (Theta)0 and (Theta)1 value. \n",
    "\n",
    "- This entire process is known as an Optimization Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6968b-fdcb-4f9f-8c80-b9211fbdc3cb",
   "metadata": {},
   "source": [
    "## Optimization Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93562bf-637e-42b8-b453-893f58066b08",
   "metadata": {},
   "source": [
    "- Deriving the cost function\n",
    "\n",
    "\n",
    "As discussed in the previous section, to get the best fit line, we minimize the error. Cost function does this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa0d7c-89f2-4568-af1a-29d0a42883b0",
   "metadata": {},
   "source": [
    "J{(theta)0 , (theta)1} : Our main aim is to minimize this error of J."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e3696-fba0-4660-b804-5ea954f64f5f",
   "metadata": {},
   "source": [
    "- While calculating the cost function(error), we get a curve in a graph (explained in the video 44.21 min). Our main aim is to come to the lowest point of the curve by calculating the cost function again and again repeatedly. This lowest point is known as global minima. In this global minima, our cost function will be less which we decide to choose. This is the point we should stop and draw our best fit line. This is the point where the error will be minimized. \n",
    "\n",
    "- We call this curve as Gradient Decent Curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebc1d9-43ae-4e03-b049-c3d05e86c85d",
   "metadata": {},
   "source": [
    "## Convergence Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c5a14-ed4e-438c-900a-0fe3e1377820",
   "metadata": {},
   "source": [
    "It is not possible to continuesly inistialize the (theta)0 and (theta)1 value. There should be a method, where we can initialize a value, and it moves towards the global minima through optimization. Convergence Algorithm plays a role to do this process. It optimizes the change of (theta)1. \n",
    "\n",
    "- The basic algorithm we write: We repeat until convergence. Convergence means, we are coming towards the global minima. The equation is explained in the video (48min). \n",
    "- The alpha value of the equation is known as Learning Rate.\n",
    "- As we know, we are getting a curve in the graph representation while calculating the j(error). This curve is in a shape of parabola. At any point of this curve, if we are calculating the (Theta)j, we need to calculate the derivative or slop of that particular point of parabola curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd0110-b5ba-49f3-baf9-33b36a064b5a",
   "metadata": {},
   "source": [
    "## Learning Rate (Interview Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a4f04-b96e-40d0-91fd-0026a5834bae",
   "metadata": {},
   "source": [
    "It defines the speed of the convergence. \n",
    "- If the learning rate value is large, we will never get to the global minima. \n",
    "- If the learning rate value is very small, then the convergence will take alot of time. \n",
    "\n",
    "We should select an ideal learning rate in this case. \n",
    "- Learning rate decides the convergance speed. (Very important interview question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31328cec-b2d3-4982-b3db-22b6785c57c2",
   "metadata": {},
   "source": [
    "So overall idea is, we need to repeat this entire convergance algorithm unless and untill we reach the global minima. If we are reaching on a global minima, there, our cost function (j) is very very minimum, and we will be getting automatically (Theta)0 and (theta)1 value. Based on this, we can create our best fit line. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f06479-9535-4f68-bd61-03c8fbfa537e",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959f137-c944-4757-8507-0b5543659dcb",
   "metadata": {},
   "source": [
    "- Gradient Decent is an optimizer function that will keep on changing our theta values.\n",
    "- Convergance algorithm is \"Repeat untill convergance\".\n",
    "- Cost function's another name is, MEAN SQUARED ERROR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69325b-0488-4f88-894b-d9bc012e1bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
